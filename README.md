# Awesome-Neuroscience-Agent-Reasoning
Neuroscience Inspired Agent Reasoning Framework

## ðŸ“¢ News

## Agentic Reasoning: A Novel Systematic Framework and Survey from Neuroscience Perspective

![image](./figures/reasoning_process.png)

## Latest Reasoning Surveys
* **Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey (arXiv 2025)** [[Paper]](https://arxiv.org/abs/2503.12605) [[Code]](https://github.com/yaotingwangofficial/Awesome-MCoT)
* **Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models (arXiv 2025)** [[Paper]](https://arxiv.org/abs/2503.16419) [[Code]](https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs)
* **From System 1 to System 2: A Survey of Reasoning Large Language Models (arXiv 2025)** [[Paper]](https://arxiv.org/abs/2502.17419) [[Code]](https://github.com/zzli2022/Awesome-System2-Reasoning-LLM)
* **A Survey of Reasoning with Foundation Models (arXiv 2023)** [[Paper]](https://arxiv.org/abs/2312.11562) [[Code]](https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models)



## Agent Reasoning Framework
- [Dimension-based Reasoning](#dimension-based-reasoning)
  - [Part 1: Spatial Reasoning](#part-1-spatial-reasoning)
  - [Part 2: Temporal Reasoning](#part-2-temporal-reasoning)
- [Perception-based Reasoning](#perception-based-reasoning)
  - [Part 3: Visual Reasoning](#part-3-visual-reasoning)
  - [Part 4: Lingual Reasoning](#part-4-lingual-reasoning)
  - [Part 5: Auditory Reasoning](#part-5-auditory-reasoning)
  - [Part 6: Tactile Reasoning](#part-6-tactile-reasoning)
- [Interaction-based Reasoning](#interection-based-reasoning)  
  - [Part 7: Reasoning based on Agent-Agent Interaction](#part-7-reasoning-based-on-agent-agent-interaction)
  - [Part 8: Reasoning based on Agent-Human Interaction](#part-8-reasoning-based-on-agent-human-interaction)
- [Logic-based Reasoning](#logic-based-reasoning)
  - [Part 9: Inductive Reasoning](#part-9-inductive-reasoning)
  - [Part 10: Deductive Reasoning](#part-10-deductive-reasoning)
  - [Part 11: Abductive Reasoning](#part-11-abductive-reasoning)
- [Evaluation&Benchmark](#evaluation-benchmark)
## Dimension-based Reasoning
## Part 1: Spatial Reasoning
* **Visual Spatial Reasoning (TACL 2023)** [[Paper]](https://doi.org/10.1162/tacl_a_00566)
* **SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities (CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Chen_SpatialVLM_Endowing_Vision-Language_Models_with_Spatial_Reasoning_Capabilities_CVPR_2024_paper.pdf))
* **Large Language Models are Visual Reasoning Coordinators (NeurIPS 2023)** [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/ddfe6bae7b869e819f842753009b94ad-Paper-Conference.pdf)
* **Is a Picture Worth a Thousand Words? Delving into Spatial Reasoning for Vision-Language Models (NeurIPS 2024)** [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/89cc5e613d34f90de90c21e996e60b30-Paper-Conference.pdf)
* **Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs (CVPR 2024)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Ranasinghe_Learning_to_Localize_Objects_Improves_Spatial_Reasoning_in_Visual-LLMs_CVPR_2024_paper.pdf)
* **Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark (AAAI 2024)** [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/29811/31406)
* **SpatialPIN: Enhancing Spatial Reasoning Capabilities of Vision-Language Models through Prompting and Interacting 3D Priors (NeurIPS 2024)** [[Paper]](https://arxiv.org/pdf/2403.13438)
* **SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models (NeurIPS 2024)** [[Paper]](https://arxiv.org/pdf/2406.01584)
* **Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation (AAAI 2025)** [[Paper]](https://arxiv.org/pdf/2408.15876)
* **Metric Reasoning in Large Language Models (ACM GIS 2024)** [[Paper]](https://dl.acm.org/doi/abs/10.1145/3678717.3691226)
* **Weakly-supervised 3D Spatial Reasoning for Text-based Visual Question Answering (IEEE TIP 2023)** [[Paper]](https://ieeexplore.ieee.org/abstract/document/10141570)
* **Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models (DMLR @ICLR 2024)** [[Paper]](https://arxiv.org/pdf/2308.09778)
* **StarCraftImage: A Dataset for Prototyping Spatial Reasoning Methods for Multi-Agent Environments (CVPR 2023)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Kulinski_StarCraftImage_A_Dataset_for_Prototyping_Spatial_Reasoning_Methods_for_Multi-Agent_CVPR_2023_paper.pdf)

## Part 2: Temporal Reasoning
### LLM based
 * **Text-to-ECG: 12-Lead Electrocardiogram Synthesis Conditioned on Clinical Text Reports (ICASSP 2023)** [[Paper]](https://arxiv.org/abs/2303.09395)
 * **Can Brain Signals Reveal Inner Alignment with Human Languages (EMNLP 2023 Findings)** [[Paper]](https://arxiv.org/abs/2208.06348) [[Code]](https://github.com/Jielin-Qiu/EEG_Language_Alignment)
 * **TempoGPT: EnhancingÂ Temporal ReasoningÂ via Quantizing Embedding (arXiv 2025)** [[Paper]](https://arxiv.org/abs/2501.07335) [[Code]](https://github.com/zhanghaochuan20/TempoGPT)
 * **PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting(IEEE TKDE 2023)** [[Paper]](https://arxiv.org/abs/2210.08964)
 * **Large Language Models Can Learn Temporal Reasoning (ACL 2024)** [[Paper]](https://arxiv.org/abs/2401.06853)
 * **Back to the future: Towards explainableÂ temporal reasoningÂ with large language models (WWW 2024)** [[Paper]](https://dl.acm.org/doi/abs/10.1145/3589334.3645376)
 * **Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question Answering (EMNLP 2024 Findings)** [[Paper]](https://www.arxiv.org/abs/2409.16909)
 * **Temporal Reasoning Transfer from Text to Video (ICLR 2025)** [[Paper]](https://arxiv.org/abs/2410.06166) [[Code]](https://video-t3.github.io/)

### Graph based
 * **Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs (ICML 2017)** [[Paper]](https://arxiv.org/abs/1705.05742)
 * **Event Graph Guided Compositional Spatial-Temporal ReasoningÂ for Video Question Answering (IEEE TIP 2024)** [[Paper]](https://ieeexplore.ieee.org/abstract/document/10418133/) [[Code]](https://github.com/ByZ0e/HSTT)
 * **TemporalÂ knowledge graphÂ reasoningÂ with historical contrastive learning (AAAI 2023)** [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/25601)
 * **TemporalÂ inductive path neural network forÂ temporalÂ knowledge graphÂ reasoning (Artificial Intelligence 2024)** [[Paper]](https://www.sciencedirect.com/science/article/pii/S0004370224000213)
 * **An improvingÂ reasoningÂ network for complex question answering overÂ temporalÂ knowledge graphs (Applied Intelligence 2023)** [[Paper]](https://link.springer.com/article/10.1007/s10489-022-03913-6)
 ## Perception-based Reasoning
 ## Part 3: Visual Reasoning
### VLM based
  * **GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering (arXiv 2024)** [[Paper]](https://arxiv.org/pdf/2402.02503)
  * **Lisa:Â ReasoningÂ segmentation via largeÂ languageÂ model (CVPR 2024)** [[Paper]](http://openaccess.thecvf.com/content/CVPR2024/papers/Lai_LISA_Reasoning_Segmentation_via_Large_Language_Model_CVPR_2024_paper.pdf)
  * **KN-VLM: KNowledge-guided Vision-and-Language Model for visual abductive reasoning (Research Square 2025)** [[Paper]](https://www.researchsquare.com/article/rs-4934011/latest.pdf)
  * **Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge (ECCV 2024)** [[Paper]](https://arxiv.org/pdf/2401.10712)
### LLM based
  * **Large language models areÂ visual reasoningÂ coordinators (NeurIPS 2023)** [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2023/file/ddfe6bae7b869e819f842753009b94ad-Paper-Conference.pdfï¼‰
  * **Enhancing LLM Reasoning via Vision-Augmented Prompting (NeurIPS 2024)** [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/328c922d068dd4ccb23cec5c64e6c7fc-Paper-Conference.pdf)
  * **Improving zero-shot visual question answering via large language models with reasoning question prompts (ACM 2023)** [[Paper]](https://dl.acm.org/doi/pdf/10.1145/3581783.3612389)
  * **Visual Chain-of-Thought Prompting for Knowledge-Based Visual Reasoning (AAAI 2024)** [[Paper]](https://ojs.aaai.org/index.php/AAAI/article/view/27888/27801)
  * **Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning (NeurIPS 2024)** [[Paper]](https://proceedings.neurips.cc/paper_files/paper/2024/file/0ff38d72a2e0aa6dbe42de83a17b2223-Paper-Datasets_and_Benchmarks_Track.pdf)
  * **Visual chain of thought: bridging logical gaps with multimodal infillings (arXiv 2023)** [[Paper]](https://arxiv.org/pdf/2305.02317)
  * **End-to-End Chart Summarization via Visual Chain-of-Thought in Vision-Language Models (arXiv 2025)** [[Paper]](https://arxiv.org/pdf/2502.17589)
  * **Llava-o1: Let vision language models reason step-by-step (arXiv 2024)** [[Paper]](https://arxiv.org/pdf/2411.10440?)
### Neuro-symbolic based
  * **ExoViP: Step-by-step Verification and Exploration with Exoskeleton Modules for Compositional Visual Reasoning (COLM 2024)** [[Paper]](https://arxiv.org/pdf/2408.02210)
  * **VisualÂ programming: CompositionalÂ visual reasoningÂ without training (CVPR 2023)** [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf)
  * **Vipergpt: Visual inference via python execution for reasoning (CVPR 2023)** [[Paper]](http://openaccess.thecvf.com/content/ICCV2023/papers/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.pdf)
### RL based
  * **HYDRA: A Hyper Agent for Dynamic Compositional Visual Reasoning (ECCV 2024)** [[Paper]](https://arxiv.org/pdf/2403.12884)
  * **Vision-r1: Incentivizing reasoning capability in multimodal large language models (arXiv 2025)** [[Paper]](https://arxiv.org/pdf/2503.06749) [[Code]](https://github.com/Osilly/Vision-R1)
  * **Visual-rft: Visual reinforcement fine-tuning (arXiv 2025)** [[Paper]](https://arxiv.org/pdf/2503.01785?)
  * **Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning (arXiv 2025)** [[Paper]](https://arxiv.org/pdf/2502.19634)
  * **VLM-RL: A Unified Vision Language Models and Reinforcement Learning Framework for Safe Autonomous Driving (arXiv 2024)** [[Paper]](https://arxiv.org/pdf/2412.15544) [[Code]](https://zilin-huang.github.io/VLM-RL-website)
 ## Part 4: Lingual Reasoning

 ## Part 5: Auditory Reasoning

 ## Part 6: Tactile Reasoning
 ## Interaction-based Reasoning
 ## Part 7: Reasoning based on Agent-Agent Interaction

 ## Part 8: Reasoning based on Agent-Human Interaction
 ## Logic-based Reasoning
 ## Part 9: Inductive Reasoning

 ## Part 10: Deductive Reasoning

 ## Part 11: Abductive Reasoning
 
 ## Evaluation&Benchmark
